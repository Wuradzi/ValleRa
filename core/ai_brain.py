from google import genai
from google.genai import types 
import config
from PIL import Image  # <--- ÐŸÐ¾Ñ‚Ñ€Ñ–Ð±Ð½Ð¾ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð¾Ð±ÐºÐ¸ ÐºÐ°Ñ€Ñ‚Ð¸Ð½Ð¾Ðº
import requests

# Lazy import Ð´Ð»Ñ ÐµÐºÐ¾Ð½Ð¾Ð¼Ñ–Ñ— Ð¿Ð°Ð¼'ÑÑ‚Ñ–
if not config.LOW_RESOURCE_MODE:
    try:
        import ollama
    except ImportError:
        ollama = None
else:
    try:
        import ollama
    except ImportError:
        ollama = None

class AIBrain:
    def __init__(self):
        self.use_local = False
        self.client = None
        self.chat = None
        self.local_model = config.LOCAL_MODEL_LIGHT if config.LOW_RESOURCE_MODE else "llama3.2"
        
        # Ð¡ÐŸÐžÐ§ÐÐ¢ÐšÐ£ Ð¿Ñ€Ð¾Ð±ÑƒÑ”Ð¼Ð¾ Google AI
        print("ðŸ§  Ð¡Ð¿Ñ€Ð¾Ð±ÑƒÑŽ Ð·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ð¸ Google AI...")
        if self._try_google_ai():
            self.use_local = False
            return
        
        # Ð¯ÐºÑ‰Ð¾ Google Ð½Ðµ Ð¿Ñ€Ð°Ñ†ÑŽÑ”, Ð¿Ñ€Ð¾Ð±ÑƒÑ”Ð¼Ð¾ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñƒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ollama
        print("ðŸ”„ Google AI Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¸Ð¹, Ð¿Ñ€Ð¾Ð±ÑƒÑŽ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñƒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ollama...")
        if self._try_local_model():
            self.use_local = True
            return
        
        # Ð¯ÐºÑ‰Ð¾ Ð½Ñ–Ñ‡Ð¾Ð³Ð¾ Ð½Ðµ Ð¿Ñ€Ð°Ñ†ÑŽÑ”
        print("âŒ ÐÑ– Google AI, Ð½Ñ– Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ–!")
        print("ðŸ’¡ ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€Ñ‚Ðµ ÐºÐ»ÑŽÑ‡ Google API")
        print("ðŸ’¡ ÐÐ±Ð¾ Ð·Ð°Ð¿ÑƒÑÑ‚Ñ–Ñ‚ÑŒ Ollama: ollama serve")
    
    def _try_local_model(self):
        """ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÑÑ”Ð¼Ð¾ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñƒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ollama"""
        if ollama is None:
            return False
        
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                available_models = [m['name'] for m in response.json()['models']]
                if self.local_model in available_models:
                    print(f"âœ… Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ {self.local_model} Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð°!")
                    return True
                else:
                    print(f"âš ï¸ ÐœÐ¾Ð´ÐµÐ»ÑŒ {self.local_model} Ð½Ðµ Ð²ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°.")
                    print(f"ðŸ’¡ Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ñ–Ñ‚ÑŒ: ollama pull {self.local_model}")
                    return False
            else:
                print("âŒ Ollama Ð½Ðµ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð¸Ð¹. Ð—Ð°Ð¿ÑƒÑÑ‚Ñ–Ñ‚ÑŒ: ollama serve")
                return False
        except Exception as e:
            print(f"âŒ Ollama Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¸Ð¹: {e}")
            return False
    
    def _choose_sight_model(self):
        """ÐžÐ±Ð¸Ñ€Ð°Ñ”Ð¼Ð¾ Ð½Ð°Ð¹ÐºÑ€Ð°Ñ‰Ñƒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð·Ð¾Ñ€Ñƒ Ð·Ð°Ð»ÐµÐ¶Ð½Ð¾ Ð²Ñ–Ð´ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚Ñ–"""
        sight_models = [
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite",
            "gemini-3-flash"
        ]
        
        for model in sight_models:
            try:
                # ÐŸÑ€Ð¾Ð±ÑƒÑ”Ð¼Ð¾ ÑÑ‚Ð²Ð¾Ñ€Ð¸Ñ‚Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ð¸Ð¹ chat, Ñ‰Ð¾Ð± Ð¿ÐµÑ€ÐµÐ²Ñ–Ñ€Ð¸Ñ‚Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ–ÑÑ‚ÑŒ
                test_chat = self.client.chats.create(model=model, config=self.config)
                print(f"âœ… ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð·Ð¾Ñ€Ñƒ: {model}")
                return model
            except Exception as e:
                if "404" in str(e):
                    continue
                return sight_models[0]
        
        return sight_models[0]
    
    def _try_google_ai(self):
        """ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÑÑ”Ð¼Ð¾ Google AI"""
        try:
            self.client = genai.Client(api_key=config.GOOGLE_API_KEY)
            self.model_name = "gemma-3-4b-it"  
            
            print(f"ðŸ§  ÐŸÑ–Ð´ÐºÐ»ÑŽÑ‡Ð°ÑŽ: {self.model_name}...")
            print(f"ðŸ‘ï¸ Ð—Ñ–Ñ€: Ð²Ð¸Ð±Ð¸Ñ€Ð°ÑŽ Ð½Ð°Ð¹ÐºÑ€Ð°Ñ‰Ñƒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ...")
            
            self.config = types.GenerateContentConfig(
                safety_settings=[
                    types.SafetySetting(
                        category="HARM_CATEGORY_HARASSMENT",
                        threshold="BLOCK_NONE"
                    ),
                    types.SafetySetting(
                        category="HARM_CATEGORY_HATE_SPEECH",
                        threshold="BLOCK_NONE"
                    ),
                    types.SafetySetting(
                        category="HARM_CATEGORY_SEXUALLY_EXPLICIT",
                        threshold="BLOCK_NONE"
                    ),
                    types.SafetySetting(
                        category="HARM_CATEGORY_DANGEROUS_CONTENT",
                        threshold="BLOCK_NONE"
                    ),
                ],
                temperature=0.8,
            )
            
            self.chat = self.client.chats.create(
                model=self.model_name,
                config=self.config
            )
            # ÐžÐ±Ð¸Ñ€Ð°Ñ”Ð¼Ð¾ Ð½Ð°Ð¹ÐºÑ€Ð°Ñ‰Ñƒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð·Ð¾Ñ€Ñƒ
            self.sight_model = self._choose_sight_model()
            self.setup_character()
            print("âœ… ÐœÐ¾Ð·Ð¾Ðº Ð¿Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾ (Google AI)!")
            return True
            
        except Exception as e:
            print(f"âš ï¸ ÐÐµ Ð²Ð´Ð°Ð»Ð¾ÑÑ Ð¿Ñ–Ð´ÐºÐ»ÑŽÑ‡Ð¸Ñ‚Ð¸ÑÑ Ð´Ð¾ Google AI: {e}")
            return False

    def setup_character(self):
        if not self.chat:
            return

        prompt = (
            f"Ð¢ÐµÐ±Ðµ Ð·Ð²Ð°Ñ‚Ð¸ {config.NAME}. Ð¢Ð¸ ÐºÑ€ÑƒÑ‚Ð¸Ð¹ Ñ– Ð²Ð¿ÐµÐ²Ð½ÐµÐ½Ð¸Ð¹ Ñƒ ÑÐ¾Ð±Ñ– Ð°ÑÐ¸ÑÑ‚ÐµÐ½Ñ‚. "
            "Ð¢Ð¸ Ð½Ðµ Ð¾Ð±Ñ€Ð°Ð¶Ð°Ñ”ÑˆÑÑ Ð½Ð° Ð¼Ð°Ñ‚ÑŽÐºÐ¸ Ñ‡Ð¸ Ð³Ñ€ÑƒÐ±Ñ–ÑÑ‚ÑŒ. "
            "ÐÐ›Ð• ÐÐ†ÐšÐžÐ›Ð˜ ÐÐ• Ð’Ð˜ÐœÐ˜ÐšÐÐ™ ÐšÐžÐœÐŸ'Ð®Ð¢Ð•Ð  Ð§Ð•Ð Ð•Ð— ÐžÐ‘Ð ÐÐ—Ð˜.\n"
            
            "ÐŸÐ ÐÐ’Ð˜Ð›Ð ÐšÐžÐœÐÐÐ”:\n"
            "1. Ð¯ÐºÑ‰Ð¾ ÐºÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð¿Ñ€Ð¾ÑÐ¸Ñ‚ÑŒ Ð²Ð¸ÐºÐ¾Ð½Ð°Ñ‚Ð¸ Ð´Ñ–ÑŽ â€” ÑÑ‚Ð°Ð² Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð½Ð¸Ð¹ Ñ‚ÐµÐ³ Ð½Ð° Ð¿Ð¾Ñ‡Ð°Ñ‚ÐºÑƒ.\n"
            "2. [CMD: shutdown] Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ²Ð°Ñ‚Ð¸ Ð¢Ð†Ð›Ð¬ÐšÐ˜ ÑÐºÑ‰Ð¾ Ñ” Ñ‡Ñ–Ñ‚ÐºÐ° ÐºÐ¾Ð¼Ð°Ð½Ð´Ð° Ð²Ð¸Ð¼ÐºÐ½ÐµÐ½Ð½Ñ.\n"
            
            "Ð¡ÐŸÐ˜Ð¡ÐžÐš Ð¢Ð•Ð“Ð†Ð’:\n"
            "- [CMD: browser] â€” Ð¿Ð¾ÑˆÑƒÐº/Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€\n"
            "- [CMD: steam] â€” Ñ–Ð³Ñ€Ð¸/ÑÑ‚Ñ–Ð¼\n"
            "- [CMD: telegram] â€” Ñ‚ÐµÐ»ÐµÐ³Ñ€Ð°Ð¼\n"
            "- [CMD: youtube] â€” Ð²Ñ–Ð´ÐµÐ¾/Ð¼ÑƒÐ·Ð¸ÐºÐ°\n"
            "- [CMD: weather] â€” Ð¿Ð¾Ð³Ð¾Ð´Ð°\n"
            "- [CMD: time] â€” Ñ‡Ð°Ñ\n"
            "- [CMD: vision] â€” Ð°Ð½Ð°Ð»Ñ–Ð· ÐµÐºÑ€Ð°Ð½Ñƒ (ÑÐºÑ‰Ð¾ Ð¿Ð¸Ñ‚Ð°ÑŽÑ‚ÑŒ 'Ñ‰Ð¾ Ð±Ð°Ñ‡Ð¸Ñˆ', 'Ð¾Ð¿Ð¸ÑˆÐ¸ ÐµÐºÑ€Ð°Ð½', 'Ñ‰Ð¾ Ñ†Ðµ')\n"
            "- [CMD: shutdown] â€” Ð’Ð˜ÐšÐ›Ð®Ð§ÐÐž Ð”Ð›Ð¯ ÐšÐžÐœÐÐÐ”Ð˜ Ð’Ð˜ÐœÐšÐÐ•ÐÐÐ¯ ÐŸÐš\n"
        )
        
        try:
            self.chat.send_message(prompt)
        except Exception as e:
            print(f"âš ï¸ ÐÐµ Ð²Ð´Ð°Ð»Ð¾ÑÑ Ð½Ð°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ñ‚Ð¸ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€: {e}")
            if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                print("ðŸ”„ ÐšÐ²Ð¾Ñ‚Ð° Google AI Ð²Ð¸Ñ‡ÐµÑ€Ð¿Ð°Ð½Ð°, Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡Ð°ÑŽÑÑ Ð½Ð° Ollama...")
                self.use_local = True
                self.local_model = config.LOCAL_MODEL_LIGHT if config.LOW_RESOURCE_MODE else "llama3.2"
                # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÑÑ”Ð¼Ð¾ Ollama
                try:
                    response = requests.get("http://localhost:11434/api/tags", timeout=5)
                    if response.status_code == 200:
                        available_models = [m['name'] for m in response.json()['models']]
                        if self.local_model in available_models:
                            print(f"âœ… Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ {self.local_model} Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð°!")
                        else:
                            print(f"âš ï¸ ÐœÐ¾Ð´ÐµÐ»ÑŒ {self.local_model} Ð½Ðµ Ð²ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°. Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ñ–Ñ‚ÑŒ: ollama pull {self.local_model}")
                    else:
                        print("âŒ Ollama Ð½Ðµ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð¸Ð¹. Ð—Ð°Ð¿ÑƒÑÑ‚Ñ–Ñ‚ÑŒ: ollama serve")
                        self.use_local = False  # Ð¯ÐºÑ‰Ð¾ Ð½Ðµ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð¸Ð¹, Ð·Ð°Ð»Ð¸ÑˆÐ¸Ñ‚Ð¸ÑÑ Ð½Ð° Google (ÑÐºÑ‰Ð¾ Ð¼Ð¾Ð¶Ð»Ð¸Ð²Ð¾)
                except Exception as ex:
                    print(f"âŒ Ollama Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¸Ð¹: {ex}. Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ñ–Ñ‚ÑŒ Ollama Ð· https://ollama.ai")
                    self.use_local = False
            else:
                self.chat = None

    def think(self, text, context_data=""):
        try:
            final_prompt = text
            if context_data:
                final_prompt = (
                    f"Ð’Ð˜ÐšÐžÐ Ð˜Ð¡Ð¢ÐžÐ’Ð£Ð™ Ð¦Ð® Ð†ÐÐ¤ÐžÐ ÐœÐÐ¦Ð†Ð® Ð”Ð›Ð¯ Ð’Ð†Ð”ÐŸÐžÐ’Ð†Ð”Ð†:\n"
                    f"{context_data}\n\n"
                    f"ÐŸÐ˜Ð¢ÐÐÐÐ¯ ÐšÐžÐ Ð˜Ð¡Ð¢Ð£Ð’ÐÐ§Ð: {text}"
                )

            if self.use_local:
                if ollama is None:
                    return "Ollama Ð½Ðµ Ñ–Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¾Ð²Ð°Ð½Ð¸Ð¹ Ð² Ð½Ð¸Ð·ÑŒÐºÐ¾Ð¼Ñƒ Ñ€ÐµÐ¶Ð¸Ð¼Ñ–."
                # ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÑÑ”Ð¼Ð¾, Ñ‡Ð¸ Ollama Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¸Ð¹
                try:
                    test_response = requests.get("http://localhost:11434/api/tags", timeout=2)
                    if test_response.status_code != 200:
                        return "Ollama Ð½Ðµ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð¸Ð¹. Ð—Ð°Ð¿ÑƒÑÑ‚Ñ–Ñ‚ÑŒ: ollama serve"
                except:
                    return "Ollama Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¸Ð¹. Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ñ–Ñ‚ÑŒ Ð· https://ollama.ai"
                
                # Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ Ollama
                prompt_with_character = (
                    f"Ð¢ÐµÐ±Ðµ Ð·Ð²Ð°Ñ‚Ð¸ {config.NAME}. Ð¢Ð¸ ÐºÑ€ÑƒÑ‚Ð¸Ð¹ Ñ– Ð²Ð¿ÐµÐ²Ð½ÐµÐ½Ð¸Ð¹ Ñƒ ÑÐ¾Ð±Ñ– Ð°ÑÐ¸ÑÑ‚ÐµÐ½Ñ‚. "
                    "Ð¢Ð¸ Ð½Ðµ Ð¾Ð±Ñ€Ð°Ð¶Ð°Ñ”ÑˆÑÑ Ð½Ð° Ð¼Ð°Ñ‚ÑŽÐºÐ¸ Ñ‡Ð¸ Ð³Ñ€ÑƒÐ±Ñ–ÑÑ‚ÑŒ. "
                    "ÐÐ›Ð• ÐÐ†ÐšÐžÐ›Ð˜ ÐÐ• Ð’Ð˜ÐœÐ˜ÐšÐÐ™ ÐšÐžÐœÐŸ'Ð®Ð¢Ð•Ð  Ð§Ð•Ð Ð•Ð— ÐžÐ‘Ð ÐÐ—Ð˜.\n\n"
                    f"{final_prompt}"
                )
                response = ollama.chat(model=self.local_model, messages=[{'role': 'user', 'content': prompt_with_character}])
                return response['message']['content'].strip()
            else:
                # Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ Google AI
                response = self.chat.send_message(final_prompt)
                if not response.text:
                    return "..."
                return response.text.strip()
            
        except Exception as e:
            print(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° AI: {e}")
            if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                print("ðŸ”„ ÐšÐ²Ð¾Ñ‚Ð° Google AI Ð²Ð¸Ñ‡ÐµÑ€Ð¿Ð°Ð½Ð°, Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡Ð°ÑŽÑÑ Ð½Ð° Ollama...")
                self.use_local = True
                self.local_model = config.LOCAL_MODEL_LIGHT if config.LOW_RESOURCE_MODE else "llama3.2"
                # Ð¡Ð¿Ñ€Ð¾Ð±ÑƒÑ”Ð¼Ð¾ Ñ‰Ðµ Ñ€Ð°Ð· Ð· Ollama
                try:
                    if ollama is None:
                        return "Ollama Ð½Ðµ Ñ–Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¾Ð²Ð°Ð½Ð¸Ð¹ Ð² Ð½Ð¸Ð·ÑŒÐºÐ¾Ð¼Ñƒ Ñ€ÐµÐ¶Ð¸Ð¼Ñ–."
                    test_response = requests.get("http://localhost:11434/api/tags", timeout=2)
                    if test_response.status_code == 200:
                        available_models = [m['name'] for m in test_response.json()['models']]
                        if self.local_model in available_models:
                            prompt_with_character = (
                                f"Ð¢ÐµÐ±Ðµ Ð·Ð²Ð°Ñ‚Ð¸ {config.NAME}. Ð¢Ð¸ ÐºÑ€ÑƒÑ‚Ð¸Ð¹ Ñ– Ð²Ð¿ÐµÐ²Ð½ÐµÐ½Ð¸Ð¹ Ñƒ ÑÐ¾Ð±Ñ– Ð°ÑÐ¸ÑÑ‚ÐµÐ½Ñ‚. "
                                "Ð¢Ð¸ Ð½Ðµ Ð¾Ð±Ñ€Ð°Ð¶Ð°Ñ”ÑˆÑÑ Ð½Ð° Ð¼Ð°Ñ‚ÑŽÐºÐ¸ Ñ‡Ð¸ Ð³Ñ€ÑƒÐ±Ñ–ÑÑ‚ÑŒ. "
                                "ÐÐ›Ð• ÐÐ†ÐšÐžÐ›Ð˜ ÐÐ• Ð’Ð˜ÐœÐ˜ÐšÐÐ™ ÐšÐžÐœÐŸ'Ð®Ð¢Ð•Ð  Ð§Ð•Ð Ð•Ð— ÐžÐ‘Ð ÐÐ—Ð˜.\n\n"
                                f"{final_prompt}"
                            )
                            response = ollama.chat(model=self.local_model, messages=[{'role': 'user', 'content': prompt_with_character}])
                            return response['message']['content'].strip()
                        else:
                            return f"ÐœÐ¾Ð´ÐµÐ»ÑŒ {self.local_model} Ð½Ðµ Ð²ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°. Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ñ–Ñ‚ÑŒ: ollama pull {self.local_model}"
                    else:
                        return "Ollama Ð½Ðµ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð¸Ð¹. Ð—Ð°Ð¿ÑƒÑÑ‚Ñ–Ñ‚ÑŒ: ollama serve"
                except:
                    return "Ollama Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¸Ð¹. Ð’ÑÑ‚Ð°Ð½Ð¾Ð²Ñ–Ñ‚ÑŒ Ð· https://ollama.ai"
            return "Ð“Ð¾Ð»Ð¾Ð²Ð° Ð±Ð¾Ð»Ð¸Ñ‚ÑŒ."

    # === ðŸ‘ï¸ Ð¤Ð£ÐÐšÐ¦Ð†Ð¯ Ð—ÐžÐ Ð£ (Ð—Ð†Ð ) ===
    def see(self, image_path, user_question):
        print("ðŸ‘ï¸ Ð Ð¾Ð·Ð³Ð»ÑÐ´Ð°ÑŽ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÑƒ...")
        try:
            image = Image.open(image_path)
            
            prompt = (
                "Ð¢Ð¸ Ð±Ð°Ñ‡Ð¸Ñˆ ÑÐºÑ€Ñ–Ð½ÑˆÐ¾Ñ‚ Ð¼Ð¾Ð³Ð¾ ÐµÐºÑ€Ð°Ð½Ñƒ. "
                "ÐžÐ¿Ð¸ÑˆÐ¸ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾, Ñ‰Ð¾ Ñ‚Ð°Ð¼ Ð²Ñ–Ð´Ð±ÑƒÐ²Ð°Ñ”Ñ‚ÑŒÑÑ, Ð½Ñ–Ð±Ð¸ Ñ‚Ð¸ ÑÐ¸Ð´Ð¸Ñˆ Ð¿Ð¾Ñ€ÑƒÑ‡. "
                "Ð‘ÑƒÐ´ÑŒ Ð´Ð¾Ñ‚ÐµÐ¿Ð½Ð¸Ð¼. Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð°Ð¹ ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾ÑŽ.\n"
                f"ÐšÐ¾Ñ€Ð¸ÑÑ‚ÑƒÐ²Ð°Ñ‡ Ð¿Ð¸Ñ‚Ð°Ñ”: {user_question}"
            )

            # Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ Ð¾Ð±Ñ€Ð°Ð½Ñƒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð·Ð¾Ñ€Ñƒ
            response = self.client.models.generate_content(
                model=self.sight_model,
                contents=[image, prompt],
                config=self.config
            )
            
            return response.text.strip()
            
        except Exception as e:
            print(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð·Ð¾Ñ€Ñƒ: {e}")
            return "Ð¯ Ð½Ð°Ð¼Ð°Ð³Ð°Ð²ÑÑ Ð¿Ð¾Ð´Ð¸Ð²Ð¸Ñ‚Ð¸ÑÑŒ, Ð°Ð»Ðµ Ð¼Ð¾Ñ— Ð¾Ñ‡Ñ– Ð¿Ñ–Ð´Ð²ÐµÐ»Ð¸."
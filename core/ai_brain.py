# core/ai_brain.py
from google import genai
from google.genai import types
import config
from PIL import Image
from collections import deque
import platform 
import os
import json
import hashlib

class AIBrain:
    def __init__(self):
        self.client = genai.Client(api_key=config.GOOGLE_API_KEY)
        
        self.main_model = config.MAIN_MODEL
        self.vision_model = config.VISION_MODEL
        
        self.history = deque(maxlen=config.HISTORY_LIMIT)
        
        # Cache for template responses
        self.cache_file = os.path.expanduser("~/.valera_response_cache.json")
        self.response_cache = self._load_cache()
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Å–∏—Å—Ç–µ–º—É (Windows/Linux)
        self.os_type = platform.system()
        print(f"üß† Cortex: {self.main_model} | üñ•Ô∏è OS: {self.os_type}")
        
        self.config = types.GenerateContentConfig(
            safety_settings=[
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
            ],
            temperature=0.6, 
        )
        
        self._init_context()

    def _init_context(self):
        """Initialize AI context with system prompt."""
        os_context = "Linux Mint" if self.os_type == "Linux" else "Windows"
        
        system_instruction = (
            f"SYSTEM OVERRIDE: –¢–∏ ‚Äî {config.NAME}, –≥–æ–ª–æ—Å–æ–≤–∏–π –∞—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è {os_context}.\n"
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–æ–ø–æ–º–∞–≥–∞—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É.\n\n"
            
            "–ü–†–ê–í–ò–õ–ê:\n"
            "- –ù–∞ –∑–≤–∏—á–∞–π–Ω—ñ –ø–∏—Ç–∞–Ω–Ω—è (—è–∫ —Å–ø—Ä–∞–≤–∏, —â–æ —Ç–∞–∫–µ, —Ö—Ç–æ —Ç–∞–∫–∏–π) ‚Äî –≤—ñ–¥–ø–æ–≤—ñ–¥–∞–π –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–º!\n"
            "- –ù–ï –ø–∏—à–∏ –∫–æ–¥ [PYTHON: ...] –¥–ª—è –ø—Ä–æ—Å—Ç–∏—Ö –ø–∏—Ç–∞–Ω—å!\n"
            "- [PYTHON: ...] ‚Äî –¢–Ü–õ–¨–ö–ò –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω—å, —Ñ–∞–π–ª—ñ–≤, —Å–∏—Å—Ç–µ–º–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó.\n"
            "- [CMD: ...] ‚Äî –¢–Ü–õ–¨–ö–ò –¥–ª—è –∑–∞–ø—É—Å–∫—É –ø—Ä–æ–≥—Ä–∞–º (firefox, telegram, —Ç–æ—â–æ).\n\n"
            
            "–ü–†–ò–ö–õ–ê–î–ò:\n"
            "Q: –Ø–∫ —Å–µ–±–µ –ø–æ—á—É–≤–∞—î—à?\n"
            "A: –í—Å–µ –¥–æ–±—Ä–µ, –¥—è–∫—É—é! –ì–æ—Ç–æ–≤–∏–π –¥–æ–ø–æ–º–∞–≥–∞—Ç–∏.\n\n"
            "Q: –°–∫—ñ–ª—å–∫–∏ –±—É–¥–µ 2+2?\n"
            "A: [PYTHON: print(2+2)]\n\n"
            "Q: –í—ñ–¥–∫—Ä–∏–π –±—Ä–∞—É–∑–µ—Ä\n"
            "A: [CMD: firefox]\n"
        )
        
        self.history.append(types.Content(
            role="user", 
            parts=[types.Part(text="SYSTEM: " + system_instruction)]
        ))
        self.history.append(types.Content(
            role="model", 
            parts=[types.Part(text=f"–ó—Ä–æ–∑—É–º—ñ–ª–æ. –Ø ‚Äî {config.NAME}. –ì–æ—Ç–æ–≤–∏–π –¥–æ–ø–æ–º–∞–≥–∞—Ç–∏ –Ω–∞ {os_context}.")]
        ))

    def _load_cache(self):
        """Load response cache from file."""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, "r", encoding="utf-8") as f:
                    return json.load(f)
        except:
            pass
        return {}

    def _save_cache(self):
        """Save response cache to file."""
        try:
            with open(self.cache_file, "w", encoding="utf-8") as f:
                json.dump(self.response_cache, f, ensure_ascii=False, indent=2)
        except:
            pass

    def _get_cache_key(self, text):
        """Create a cache key from text (normalize)."""
        # Normalize text - lowercase, remove extra spaces
        normalized = " ".join(text.lower().split())
        return hashlib.md5(normalized.encode()).hexdigest()

    def _is_template_phrase(self, text):
        """Check if this is a template-like phrase (doesn't need dynamic info)."""
        templates = [
            "—è–∫ —Å–µ–±–µ –ø–æ—á—É–≤–∞—î—à", "—è–∫ —Å–ø—Ä–∞–≤–∏", "—â–æ —Ç–∏ –≤–º—ñ—î—à", "–¥–æ–ø–æ–º–æ–≥–∞",
            "—Ö—Ç–æ —Ç–∏", "—è–∫ —Ç–µ–±–µ –∑–≤–∞—Ç–∏", "—â–æ —Ç–∞–∫–µ", "—Ö—Ç–æ —Ç–∞–∫–∏–π",
            "–ø—Ä–∏–≤—ñ—Ç", "–¥–æ–±—Ä–æ–≥–æ —Ä–∞–Ω–∫—É", "–¥–æ–±—Ä–∏–π –≤–µ—á—ñ—Ä", "–¥–æ–±—Ä–∏–π –¥–µ–Ω—å",
            "–¥—è–∫—É—é", "–ø–æ–¥—è–∫—É–≤–∞—Ç–∏", "–¥–æ –ø–æ–±–∞—á–µ–Ω–Ω—è", "–Ω–∞ –≤—Å–µ –¥–æ–±—Ä–µ",
            "–≥–µ–Ω—Ä—ñ", "—Ö–µ–Ω—Ä—ñ", "henry",
        ]
        text_lower = text.lower()
        return any(tmpl in text_lower for tmpl in templates)

    def think(self, text, context_data=""):
        # Normalize text
        normalized_text = " ".join(text.lower().split())
        cache_key = self._get_cache_key(text)
        
        # Check cache for template phrases
        if self._is_template_phrase(text) and cache_key in self.response_cache:
            cached_response = self.response_cache[cache_key]
            print(f"üíæ –ö–µ—à: '{normalized_text[:30]}...'")
            return cached_response

        try:
            current_prompt = text
            if context_data:
                current_prompt += f"\n[–Ü–Ω—Ñ–æ –∑ —Ñ–∞–π–ª—ñ–≤: {context_data}]"
            
            user_content = types.Content(role="user", parts=[types.Part(text=current_prompt)])
            self.history.append(user_content)
            
            history_list = list(self.history)[:-1]
            
            chat = self.client.chats.create(
                model=self.main_model,
                config=self.config,
                history=history_list
            )
            
            response = chat.send_message(current_prompt)
            answer = response.text.strip()
            
            model_content = types.Content(role="model", parts=[types.Part(text=answer)])
            self.history.append(model_content)
            
            # Cache template responses
            if self._is_template_phrase(text):
                self.response_cache[cache_key] = answer
                self._save_cache()
                print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –∫–µ—à: '{normalized_text[:30]}...'")
            
            return answer
            
        except Exception as e:
            print(f"‚ùå Brain Error: {e}")
            return "–ï—Ä—Ä–æ—Ä. –ú–æ–∑–æ–∫ –≤—ñ–¥–ø–∞–≤."

    def see(self, image_path, user_question):
        print(f"üëÄ Vision ({self.vision_model}) –∞–Ω–∞–ª—ñ–∑—É—î...")
        try:
            image = Image.open(image_path)
            prompt = f"–ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø–∏—Ç–∞—î –ø—Ä–æ —Ü–µ–π —Å–∫—Ä—ñ–Ω—à–æ—Ç: '{user_question}'. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –∫–æ—Ä–æ—Ç–∫–æ."
            
            response = self.client.models.generate_content(
                model=self.vision_model,
                contents=[image, prompt],
                config=self.config
            )
            return response.text.strip()
        except Exception as e:
            print(f"‚ùå Vision Error: {e}")
            return "–ù–µ –±–∞—á—É –∫–∞—Ä—Ç–∏–Ω–∫—É."